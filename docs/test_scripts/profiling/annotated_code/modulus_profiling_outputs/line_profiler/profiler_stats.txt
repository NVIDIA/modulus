Timer unit: 1e-09 s

Total time: 0.0635549 s
File: /root/modulus/docs/test_scripts/profiling/annotated_code/attn.py
Function: forward at line 31

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    31                                               @profile
    32                                               def forward(self, x: torch.Tensor) -> torch.Tensor:
    33                                           
    34         8      47352.0   5919.0      0.1          B, N, C = x.shape
    35         8   41506771.0    5e+06     65.3          qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)
    36         8     250098.0  31262.2      0.4          q, k, v = qkv.unbind(0)
    37                                           
    38                                           
    39                                                   # This is not optimal code right here ...
    40         8    4760805.0 595100.6      7.5          q = q * self.scale
    41         8    7717546.0 964693.2     12.1          attn = q @ k.transpose(-2, -1)
    42         8    6836559.0 854569.9     10.8          attn = attn.softmax(dim=-1)
    43         8     467249.0  58406.1      0.7          attn = self.attn_drop(attn)
    44         8     681692.0  85211.5      1.1          x = attn @ v
    45                                           
    46         8     273367.0  34170.9      0.4          x = x.transpose(1, 2).reshape(B, N, C)
    47         8     822387.0 102798.4      1.3          x = self.proj(x)
    48         8     182694.0  22836.8      0.3          x = self.proj_drop(x)
    49         8       8381.0   1047.6      0.0          return x

Total time: 0.00505685 s
File: /root/modulus/docs/test_scripts/profiling/annotated_code/attn.py
Function: forward at line 65

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    65                                               @profile
    66                                               def forward(self, x):
    67         8     493290.0  61661.2      9.8          x = self.fc1(x)
    68         8    3648444.0 456055.5     72.1          x = self.gelu(x)
    69         8     155234.0  19404.2      3.1          x = self.drop1(x)
    70         8     437770.0  54721.2      8.7          x = self.fc2(x)
    71         8     197332.0  24666.5      3.9          x = self.gelu(x)
    72         8     120381.0  15047.6      2.4          x = self.drop2(x)
    73         8       4401.0    550.1      0.1          return x

Total time: 0.0855953 s
File: /root/modulus/docs/test_scripts/profiling/annotated_code/attn.py
Function: forward at line 104

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   104                                               @profile
   105                                               def forward(self, x: torch.Tensor) -> torch.Tensor:
   106         8   79767938.0    1e+07     93.2          x = x + self.attn(self.norm1(x))
   107         8    5821609.0 727701.1      6.8          x = x + self.mlp(self.norm2(x))
   108         8       5719.0    714.9      0.0          return x

Total time: 4.54253 s
File: /root/modulus/docs/test_scripts/profiling/annotated_code/workload.py
Function: workload at line 30

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    30                                           @profile
    31                                           def workload(cfg):
    32                                           
    33         1     304686.0 304686.0      0.0      ds = RandomNoiseDataset(cfg["shape"])
    34                                               
    35         2     202324.0 101162.0      0.0      loader = DataLoader(
    36         1        170.0    170.0      0.0          ds, 
    37         1      42921.0  42921.0      0.0          batch_size=cfg["batch_size"], 
    38         1        180.0    180.0      0.0          shuffle = True,
    39                                               )
    40                                               
    41                                               
    42                                               # Initialize the model:
    43         3   79343062.0    3e+07      1.7      model = Block(
    44         1      79202.0  79202.0      0.0          dim = cfg["shape"][-1],
    45         1      73421.0  73421.0      0.0          num_heads = cfg.model["num_heads"],
    46         1      52831.0  52831.0      0.0          qkv_bias  = cfg.model["qkv_bias"] ,
    47         1      50570.0  50570.0      0.0          attn_drop = cfg.model["attn_drop"],
    48         1      50352.0  50352.0      0.0          proj_drop = cfg.model["proj_drop"],
    49         1  109143037.0    1e+08      2.4      ).to("cuda")
    50                                               
    51         1     107952.0 107952.0      0.0      if cfg["train"]:
    52         1 2059751411.0    2e+09     45.3          opt = torch.optim.SGD(model.parameters(), lr=0.0001, momentum=0.9)
    53                                               
    54         1        270.0    270.0      0.0      times = []    
    55         2      74021.0  37010.5      0.0      with Profiler() as p:
    56         1       1521.0   1521.0      0.0          start = time.perf_counter()
    57         9 2022090217.0    2e+08     44.5          for i, batch in enumerate(loader):
    58         8      74692.0   9336.5      0.0              image = batch["image"]
    59         8   47176297.0    6e+06      1.0              image = image.to("cuda")
    60        16     364408.0  22775.5      0.0              with annotate(domain="forward", color="blue"):
    61         8   85942255.0    1e+07      1.9                  output = model(image)
    62         8     871577.0 108947.1      0.0              if cfg["train"]:
    63         8    1728264.0 216033.0      0.0                  opt.zero_grad()
    64                                                           # Compute the loss:
    65         8   24881952.0    3e+06      0.5                  loss = loss_fn(output)
    66                                                           # Do the gradient calculation:
    67        16     181082.0  11317.6      0.0                  with annotate(domain="backward", color="green"):
    68         8   50119067.0    6e+06      1.1                      loss.backward()
    69                                                               # Apply the gradients
    70         8   58985347.0    7e+06      1.3                      opt.step()
    71         8      35302.0   4412.8      0.0              p.step()
    72         8      27261.0   3407.6      0.0              end = time.perf_counter()
    73         8     352396.0  44049.5      0.0              print(f"Finished step {i} in {end - start:.4f} seconds")
    74         8       4790.0    598.8      0.0              times.append(end - start)
    75         8       6301.0    787.6      0.0              start = time.perf_counter()
    76                                           
    77         1      84802.0  84802.0      0.0      times = torch.tensor(times)
    78                                               # Drop first and last:
    79         1     117812.0 117812.0      0.0      avg_time = times[1:-1].mean()
    80                                               # compute throughput too:
    81         1     152063.0 152063.0      0.0      throughput = cfg["batch_size"] / avg_time
    82         1      60321.0  60321.0      0.0      print(f"Average time per iteration: {avg_time:.3f} ({throughput:.3f} examples / s)")

