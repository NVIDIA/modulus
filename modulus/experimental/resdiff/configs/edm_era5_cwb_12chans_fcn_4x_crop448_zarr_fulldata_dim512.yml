
#CMD: "torchrun  --nnodes=1  --nproc_per_node=8  --max_restarts=3  --rdzv_id=1   --rdzv_backend=c10d   --rdzv_endpoint=localhost:29400   train.py"   #--nnodes=$NUM_NODES   $--rdzv_endpoint=HOST_NODE_ADDR   --rdzv_backend=etcd 

CMD: "torchrun  --nproc_per_node=${SUBMIT_GPUS} --nnodes=8 --node_rank=${NODE_RANK}  --master_addr=${MASTER_ADDR} --master_port=${MASTER_PORT}  train.py"

#SELENE
HPARAMS: [
  {
  batch: 256,  #32  #512
  #batch-gpu: 4,
  augment: 0.0,
  arch: 'ddpmpp',
  precond: 'edm',
  data: '/lustre/fsw/sw_climate_fno/nbrenowitz/2023-01-24-cwb-4years.zarr',
  outdir: LOGDIR/output,
  lr: 2e-4,   #10e-4
  duration: 200,
  snap: 1,   #tick
  dump: 1,   #tick
  #transfer: None,
  resume: None,   #checkpoint
  fp16: False,
  workers: 4,  #4
  data_config: full_field_train_crop448_grid_12inchans_fcn_4outchans_4x,
  task: 'sr',                 #['sr', 'pred']
  data_type: 'era5-cwb-v3',   #['era5', 'cwb', 'era5-cwb', 'zarr_v1']
  },
]